{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6280952-a170-448b-83b2-dd1b078da622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSimple linear regression has only one x and one y variable.\\n\\nMultiple linear regression has one y and two or more x variables.\\n\\nFor instance, when we predict rent based on square feet alone that is simple linear regression.\\n\\nWhen we predict rent based on square feet and age of the building that is an example of multiple linear regression.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "Simple linear regression has only one x and one y variable.\n",
    "\n",
    "Multiple linear regression has one y and two or more x variables.\n",
    "\n",
    "For instance, when we predict rent based on square feet alone that is simple linear regression.\n",
    "\n",
    "When we predict rent based on square feet and age of the building that is an example of multiple linear regression.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8749cd93-9a71-4d81-896c-baeee7ac482a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are primarily five assumptions of linear regression. They are:\\n\\n1.There is a linear relationship between the predictors (x) and the outcome (y)\\n2.Predictors (x) are independent and observed with negligible error\\n3.Residual Errors have a mean value of zero\\n4.Residual Errors have constant variance\\n5.Residual Errors are independent from each other and predictors (x)\\nHow to Test the Assumptions of Linear Regression?\\n \\nAssumption One: Linearity of the Data\\nWe can check the linearity of the data by looking at the Residual vs Fitted plot.\\nIdeally, this plot would not have a pattern where the red line (lowes smoother) is approximately horizontal at zero.\\n\\nAssumption Two: Predictors (x) are Independent & Observed with Negligible Error\\nThe easiest way to check the assumption of independence is using the Durbin-Watson test.\\nWe can conduct this test using R’s built-in function called durbinWatsonTest on our model.\\nRunning this test will give you an output with a p-value, which will help you determine whether the assumption is met or not. \\n\\nAssumption Three: Residual Errors have a Mean Value of Zero\\nWe can easily check this assumption by looking at the same residual vs fitted plot.\\nWe would ideally want to see the red line flat on 0, which would indicate that the residual errors have a \\nmean value of zero.\\n\\nAssumption Four: Residual Errors have Constant Variance\\nWe can check this assumption using the Scale-Location plot.\\n\\nAssumption Five: Residual Errors are Independent from Each Other & Predictors (x)\\nThis assumption requires knowledge of study design or data collection in order to establish the \\nvalidity of this assumption, so we will not be covering this in this blog.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "There are primarily five assumptions of linear regression. They are:\n",
    "\n",
    "1.There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "2.Predictors (x) are independent and observed with negligible error\n",
    "3.Residual Errors have a mean value of zero\n",
    "4.Residual Errors have constant variance\n",
    "5.Residual Errors are independent from each other and predictors (x)\n",
    "How to Test the Assumptions of Linear Regression?\n",
    " \n",
    "Assumption One: Linearity of the Data\n",
    "We can check the linearity of the data by looking at the Residual vs Fitted plot.\n",
    "Ideally, this plot would not have a pattern where the red line (lowes smoother) is approximately horizontal at zero.\n",
    "\n",
    "Assumption Two: Predictors (x) are Independent & Observed with Negligible Error\n",
    "The easiest way to check the assumption of independence is using the Durbin-Watson test.\n",
    "We can conduct this test using R’s built-in function called durbinWatsonTest on our model.\n",
    "Running this test will give you an output with a p-value, which will help you determine whether the assumption is met or not. \n",
    "\n",
    "Assumption Three: Residual Errors have a Mean Value of Zero\n",
    "We can easily check this assumption by looking at the same residual vs fitted plot.\n",
    "We would ideally want to see the red line flat on 0, which would indicate that the residual errors have a \n",
    "mean value of zero.\n",
    "\n",
    "Assumption Four: Residual Errors have Constant Variance\n",
    "We can check this assumption using the Scale-Location plot.\n",
    "\n",
    "Assumption Five: Residual Errors are Independent from Each Other & Predictors (x)\n",
    "This assumption requires knowledge of study design or data collection in order to establish the \n",
    "validity of this assumption, so we will not be covering this in this blog.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76da5f1a-ef07-4539-a8e3-8e3d9ecc4b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis.\\nThe slope and the intercept define the linear relationship between two variables, and can be used \\nto estimate an average rate of change. \\nThe greater the magnitude of the slope, the steeper the line and the greater the rate of change.\\n\\nExample 1. Data were collected on the depth of a dive of penguins and the duration of\\nthe dive. The following linear model is a fairly good summary of the data, where t is the\\nduration of the dive in minutes and d is the depth of the dive in yards. The equation for\\nthe model is d t = + 0.015 2.915\\nInterpret the slope: If the duration of the dive increases by 1 minute, we predict the\\ndepth of the dive will increase by approximately 2.915 yards.\\nInterpret the intercept. If the duration of the dive is 0 seconds, then we predict the\\ndepth of the dive is 0.015 yards\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q NO. 3 :\n",
    "\"\"\"\n",
    "The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis.\n",
    "The slope and the intercept define the linear relationship between two variables, and can be used \n",
    "to estimate an average rate of change. \n",
    "The greater the magnitude of the slope, the steeper the line and the greater the rate of change.\n",
    "\n",
    "Example 1. Data were collected on the depth of a dive of penguins and the duration of\n",
    "the dive. The following linear model is a fairly good summary of the data, where t is the\n",
    "duration of the dive in minutes and d is the depth of the dive in yards. The equation for\n",
    "the model is d t = + 0.015 2.915\n",
    "Interpret the slope: If the duration of the dive increases by 1 minute, we predict the\n",
    "depth of the dive will increase by approximately 2.915 yards.\n",
    "Interpret the intercept. If the duration of the dive is 0 seconds, then we predict the\n",
    "depth of the dive is 0.015 yards\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903a761c-406d-466b-8ca3-a0a7774a0873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Gradient Descent is known as one of the most commonly used optimization algorithms to train machine\\nlearning models by means of minimizing errors between actual and expected results.\\nFurther, gradient descent is also used to train Neural Networks.\\n\\nGradient Descent is an algorithm that solves optimization problems using first-order iterations.\\nSince it is designed to find the local minimum of a differential function, gradient descent is widely used\\nin machine learning models to find the best parameters that minimize the model's cost function.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"Gradient Descent is known as one of the most commonly used optimization algorithms to train machine\n",
    "learning models by means of minimizing errors between actual and expected results.\n",
    "Further, gradient descent is also used to train Neural Networks.\n",
    "\n",
    "Gradient Descent is an algorithm that solves optimization problems using first-order iterations.\n",
    "Since it is designed to find the local minimum of a differential function, gradient descent is widely used\n",
    "in machine learning models to find the best parameters that minimize the model's cost function.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3105e9e8-84e6-43cb-b4bd-205d23b6a55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Multiple linear regression is a regression model that estimates the relationship between a quantitative\\ndependent variable and two or more independent variables using a straight line.\\nSimple linear regression has one independent variable and multiple regression has two or more.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"Multiple linear regression is a regression model that estimates the relationship between a quantitative\n",
    "dependent variable and two or more independent variables using a straight line.\n",
    "Simple linear regression has one independent variable and multiple regression has two or more.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843ab502-c0ba-4d4e-86d7-9c75001e7b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulticollinearity occurs when two or more independent variables in a data frame have a high correlation with one\\nanother in a regression model.\\nThis means that one independent variable can be predicted from another in a regression model.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "'''\n",
    "Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one\n",
    "another in a regression model.\n",
    "This means that one independent variable can be predicted from another in a regression model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc6f84c2-485d-4329-ac96-80c24acba0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPolynomial regression is a form of Linear regression where only due to the Non-linear relationship \\nbetween dependent and independent variables, we add some polynomial terms to linear regression to convert\\nit into Polynomial regression.\\n\\nPolynomial provides the best approximation of the relationship between the dependent and independent variable.\\nA Broad range of function can be fit under it.\\nPolynomial basically fits a wide range of curvature.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "'''\n",
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship \n",
    "between dependent and independent variables, we add some polynomial terms to linear regression to convert\n",
    "it into Polynomial regression.\n",
    "\n",
    "Polynomial provides the best approximation of the relationship between the dependent and independent variable.\n",
    "A Broad range of function can be fit under it.\n",
    "Polynomial basically fits a wide range of curvature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e618a25a-667d-4d67-9274-11b6623b6de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPolynomial provides the best approximation of the relationship between the dependent and independent variable.\\nA Broad range of function can be fit under it.\\nPolynomial basically fits a wide range of curvature.\\nOne or two outliers in the data might have a significant impact on the nonlinear analysis' outcomes.\\nThese are overly reliant on outliers. Furthermore, there are fewer model validation methods for detecting \\noutliers in nonlinear regression than there are for linear regression.\\nThe main advantages of polynomial fits include reasonable flexibility for data that is not too complicated, \\nand they are linear, which means the fitting process is simple.\\nThe main disadvantage is that high-degree fits can become unstable.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"\n",
    "Polynomial provides the best approximation of the relationship between the dependent and independent variable.\n",
    "A Broad range of function can be fit under it.\n",
    "Polynomial basically fits a wide range of curvature.\n",
    "One or two outliers in the data might have a significant impact on the nonlinear analysis' outcomes.\n",
    "These are overly reliant on outliers. Furthermore, there are fewer model validation methods for detecting \n",
    "outliers in nonlinear regression than there are for linear regression.\n",
    "The main advantages of polynomial fits include reasonable flexibility for data that is not too complicated, \n",
    "and they are linear, which means the fitting process is simple.\n",
    "The main disadvantage is that high-degree fits can become unstable.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
